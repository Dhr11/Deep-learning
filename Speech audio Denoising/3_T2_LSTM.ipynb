{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_T2_LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"buJmEkQtcH6K","colab_type":"text"},"source":[" \n","Denoising using RNN- LSTM\n","\n","Simple structure with 1 LSTM And followed by one dense layer taken both with 513 hidden units.\n"]},{"cell_type":"code","metadata":{"id":"J6uSSVUAhyNP","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import librosa\n","import pickle\n","import tensorflow as tf\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qXMZxN-TcXA0","colab_type":"text"},"source":["As the fileset was big, and colaboratory suffers from temporary storage, files were stored in drive and then mounted from there. "]},{"cell_type":"code","metadata":{"id":"VoN3PlxRQuX4","colab_type":"code","outputId":"03f6727b-d139-4e2b-b1ae-497495804515","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OCaX95HaZNH4","colab_type":"code","colab":{}},"source":["def getSNR(RawInp,Pred):\n","    t = min(len(RawInp),len(Pred))\n","    clippedY_True = RawInp[:t]\n","    clippedY_Pred = Pred[:t]\n","    signal  = np.dot(clippedY_True,clippedY_True.T)\n","    subtract = clippedY_True - clippedY_Pred\n","    noise = np.dot(subtract,subtract.T)\n","    \n","    return 10*math.log10(signal/noise)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jVB-StjLclSC","colab_type":"text"},"source":["the following are the path combinations for the folders"]},{"cell_type":"code","metadata":{"id":"Aju_CGV-Qphx","colab_type":"code","colab":{}},"source":["path = './gdrive/My Drive/homework3/'\n","\n","path_train = path + 'tr/'\n","path_val = path + 'v/'\n","path_test = path + 'te/'\n","\n","path_result = path + 'result/'\n","path_model = path + 'model/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-aOnQYorFfAQ","colab_type":"code","colab":{}},"source":["def load_wav(file,f=0):\n","  sn, sr=librosa.load(file, sr=None)\n","  Xdata1=librosa.stft(sn, n_fft=1024, hop_length=512)\n","  if f==1: return sn,sr,Xdata1.T\n","  return Xdata1.T\n","  \n","def numtostring(i):\n","  if i!=0 : return \"0\"*(4 - (math.floor(math.log10(i)) + 1))+str(i)\n","  else: return \"0000\"\n","\n","nums = [numtostring(i) for i in range(1200)]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUsQPb9CFhR2","colab_type":"code","colab":{}},"source":["Xtr, Str, Ntr = [], [], []\n","Xv, Sv, Nv = [], [], []\n","max_width = 513\n","max_length = 200"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"50zBEm6cRonX","colab_type":"text"},"source":["the following can be un-commented when pickle file not there.\n","As colab is slow pickle was used to load everytime."]},{"cell_type":"code","metadata":{"id":"hyBv2atMGBDX","colab_type":"code","outputId":"1a8e4cd2-de4b-425f-8f9b-fdf20bfbe998","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"\n","for i in nums:\n","  Xtr.append(load_wav(path_train + \"trx\"+i+\".wav\",0))\n","  Str.append(load_wav(path_train+ \"trs\"+i+\".wav\",0))\n","  Ntr.append(load_wav(path_train+\"trn\"+i+\".wav\",0))\n","train=[Xtr,Str,Ntr]\n","with open(path+'train.pickle', 'wb') as f:\n","  pickle.dump(train, f)\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfor i in nums:\\n  Xtr.append(load_wav(path_train + \"trx\"+i+\".wav\",0))\\n  Str.append(load_wav(path_train+ \"trs\"+i+\".wav\",0))\\n  Ntr.append(load_wav(path_train+\"trn\"+i+\".wav\",0))\\ntrain=[Xtr,Str,Ntr]\\nwith open(path+\\'train.pickle\\', \\'wb\\') as f:\\n  pickle.dump(train, f)\\n'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Ogs51JmpGB6l","colab_type":"code","outputId":"665a4694-78e9-4aab-eaf4-b48a071faf19","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"\n","snv= []\n","\n","for i in nums:  \n","  Xv.append(load_wav(path_val+\"vx\"+i+\".wav\",0))\n","  a,b,c = load_wav(path_val+\"vs\"+i+\".wav\",1)\n","  snv.append(a),Sv.append(c)  \n","  Nv.append(load_wav(path_val+\"vn\"+i+\".wav\",0))\n","val = [snv,Xv,Sv,Nv]\n","with open(path+'val.pickle', 'wb') as f:\n","  pickle.dump(val, f)\n","\"\"\"\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nsnv= []\\n\\nfor i in nums:  \\n  Xv.append(load_wav(path_val+\"vx\"+i+\".wav\",0))\\n  a,b,c = load_wav(path_val+\"vs\"+i+\".wav\",1)\\n  snv.append(a),Sv.append(c)  \\n  Nv.append(load_wav(path_val+\"vn\"+i+\".wav\",0))\\nval = [snv,Xv,Sv,Nv]\\nwith open(path+\\'val.pickle\\', \\'wb\\') as f:\\n  pickle.dump(val, f)\\n'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"TTNnldcfDCjo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oGQgK7dRGGU3","colab_type":"code","outputId":"5018da33-bc5f-4521-914e-6e9c38024fd4","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"\n","sns,srs,test_X = [], [], []\n","for i in range(400):\n","  a,b,c =load_wav(path_test+\"tex\"+numtostring(i)+\".wav\",1)\n","  sns.append(a),test_X.append(c)\n","test= [sns,test_X]\n","with open(path+'test.pickle', 'wb') as f:\n","    pickle.dump(test, f)\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nsns,srs,test_X = [], [], []\\nfor i in range(400):\\n  a,b,c =load_wav(path_test+\"tex\"+numtostring(i)+\".wav\",1)\\n  sns.append(a),test_X.append(c)\\ntest= [sns,test_X]\\nwith open(path+\\'test.pickle\\', \\'wb\\') as f:\\n    pickle.dump(test, f)\\n'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"MeKK7CTvJPmH","colab_type":"code","colab":{}},"source":["import pickle\n","with open(path+'train.pickle', 'rb') as f:\n","  loaded_train = pickle.load(f)\n","with open(path+'test.pickle', 'rb') as f:\n","  loaded_test = pickle.load(f)  \n","with open(path+'val.pickle', 'rb') as f:\n","  loaded_val = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5eCwlBBMIp6","colab_type":"code","colab":{}},"source":["def get_abs(stft_list):\n","  out = []\n","  for x in stft_list:\n","    out.append(np.abs(np.pad(x, [(0, max_length-x.shape[0]), (0, 0)],mode='constant')))\n","  return np.array(out)             \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RcAFQ-FKJRkJ","colab_type":"code","colab":{}},"source":["loaded_X,loaded_S,loaded_N = loaded_train\n","mloaded_X,mloaded_S,mloaded_N = [get_abs(x) for x in loaded_train]\n","target = 1 * (mloaded_S > mloaded_N)\n","\n","sn,test_X = loaded_test\n","snvv, val_X,val_S,val_N = loaded_val\n","mval_X,mval_S,mval_N = [get_abs(x) for x in loaded_val[1:]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRR1xz8zGLti","colab_type":"code","colab":{}},"source":["train_tot = len(loaded_X)\n","batch_size = 128\n","num_features = 513\n","num_hidden = 513\n","\n","learning_rate = 0.007\n","num_epochs = 500\n","X = tf.placeholder(tf.float32, [None, max_length, num_features])\n","Y = tf.placeholder(tf.float32, [None, max_length, num_features])\n","keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UoKtviGrGk6M","colab_type":"code","outputId":"e8ff6ca6-99dd-4b5b-df71-643d53c1960f","colab":{"base_uri":"https://localhost:8080/","height":339}},"source":["\n","lstm = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_hidden),output_keep_prob=keep_prob)\n","output, state = tf.nn.dynamic_rnn(lstm, X, dtype=tf.float32)\n","dense = tf.layers.dense(inputs=output,units=513, activation=tf.nn.sigmoid)\n","sess = tf.Session()\n","saver = tf.train.Saver()\n","loss = tf.losses.mean_squared_error(labels=Y, predictions=dense)\n","train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss=loss)\n","init = tf.global_variables_initializer()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-14-7fab2832e586>:2: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-14-7fab2832e586>:3: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From <ipython-input-14-7fab2832e586>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vkdR-rCLdHwt","colab_type":"text"},"source":["The following is the training phase with number of epochs set to be 100 initially."]},{"cell_type":"code","metadata":{"id":"r667ClPrBP0I","colab_type":"code","colab":{}},"source":["#saver.restore(sess, path_model + 'model_lstm.ckpt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8D-Vh6FjHqaP","colab_type":"code","outputId":"d631dbb1-21a4-4d3e-fe42-0ed1cbd08ff6","colab":{"base_uri":"https://localhost:8080/","height":8434}},"source":["sess.run(tf.global_variables_initializer())\n","\n","#\n","for epoch in range(num_epochs):\n","  loss_tot = 0\n","  for i in range(0, train_tot, batch_size):\n","    end = min(i + batch_size, train_tot)    \n","    batch_x = mloaded_X[i:end]\n","    batch_y = target[i:end]\n","    _, lossi = sess.run([train, loss], feed_dict={X: batch_x, Y: batch_y,keep_prob: 0.9})\n","    loss_tot += lossi\n","    \n","  print(epoch, loss_tot)\n","saver.save(sess, path_model + 'model_lstm.ckpt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 1.9160180389881134\n","1 1.1862040236592293\n","2 1.0933885648846626\n","3 1.0324436724185944\n","4 0.9769348204135895\n","5 0.9359132423996925\n","6 0.9098641574382782\n","7 0.8893876522779465\n","8 0.8650663867592812\n","9 0.8227577731013298\n","10 0.7963450998067856\n","11 0.7805962711572647\n","12 0.7981526330113411\n","13 0.7917355671525002\n","14 0.7617887929081917\n","15 0.7462948560714722\n","16 0.7331531494855881\n","17 0.7275565788149834\n","18 0.7243918403983116\n","19 0.7251408845186234\n","20 0.7178953513503075\n","21 0.7055901400744915\n","22 0.6924942433834076\n","23 0.6792283691465855\n","24 0.669565174728632\n","25 0.6610085181891918\n","26 0.6541378833353519\n","27 0.651338592171669\n","28 0.6447532027959824\n","29 0.6484431438148022\n","30 0.6514732986688614\n","31 0.6707202158868313\n","32 0.6561906151473522\n","33 0.645463727414608\n","34 0.6389500088989735\n","35 0.6411176696419716\n","36 0.646407350897789\n","37 0.6552474498748779\n","38 0.6595397591590881\n","39 0.648781094700098\n","40 0.6333590112626553\n","41 0.6213591396808624\n","42 0.6090425550937653\n","43 0.6015408001840115\n","44 0.5967862233519554\n","45 0.5941989421844482\n","46 0.5913323014974594\n","47 0.5871961042284966\n","48 0.5844423025846481\n","49 0.5814467035233974\n","50 0.5798043496906757\n","51 0.5818978548049927\n","52 0.5796764567494392\n","53 0.5783906430006027\n","54 0.5791375413537025\n","55 0.5765210315585136\n","56 0.5715665332973003\n","57 0.569343525916338\n","58 0.5695372261106968\n","59 0.5660882517695427\n","60 0.5602070428431034\n","61 0.5568005926907063\n","62 0.5562917143106461\n","63 0.5590038783848286\n","64 0.5591120906174183\n","65 0.5581721588969231\n","66 0.5549447797238827\n","67 0.5501691736280918\n","68 0.5490025766193867\n","69 0.5451899766921997\n","70 0.543926227837801\n","71 0.5422480590641499\n","72 0.5463291890919209\n","73 0.5566748231649399\n","74 0.5634096078574657\n","75 0.5558985657989979\n","76 0.5576340369880199\n","77 0.5524322018027306\n","78 0.5477042980492115\n","79 0.5419041700661182\n","80 0.5448130257427692\n","81 0.5389181412756443\n","82 0.5397420860826969\n","83 0.5380877926945686\n","84 0.5364566370844841\n","85 0.5294154435396194\n","86 0.5231389813125134\n","87 0.5194093622267246\n","88 0.5150522850453854\n","89 0.5130937360227108\n","90 0.5114129893481731\n","91 0.5115746110677719\n","92 0.511406309902668\n","93 0.513387780636549\n","94 0.5141914673149586\n","95 0.5163970068097115\n","96 0.5194042138755322\n","97 0.5211997665464878\n","98 0.5202423147857189\n","99 0.5184942409396172\n","100 0.5161968618631363\n","101 0.5130880251526833\n","102 0.5143666118383408\n","103 0.513454981148243\n","104 0.5147168226540089\n","105 0.5157037563621998\n","106 0.5098697990179062\n","107 0.5114800482988358\n","108 0.5093335770070553\n","109 0.5082976035773754\n","110 0.5050917118787766\n","111 0.5020212791860104\n","112 0.5014766901731491\n","113 0.4996414817869663\n","114 0.4957873113453388\n","115 0.4932453855872154\n","116 0.49053386598825455\n","117 0.48774081096053123\n","118 0.48609744384884834\n","119 0.484160128980875\n","120 0.4831203334033489\n","121 0.48229917883872986\n","122 0.4814564920961857\n","123 0.4814557172358036\n","124 0.4823472201824188\n","125 0.48157749325037\n","126 0.48236874490976334\n","127 0.48264016956090927\n","128 0.4821997098624706\n","129 0.4815340116620064\n","130 0.4798901006579399\n","131 0.4786961302161217\n","132 0.47896867245435715\n","133 0.47717446088790894\n","134 0.4775501377880573\n","135 0.47774994745850563\n","136 0.4779995009303093\n","137 0.477473396807909\n","138 0.47735198959708214\n","139 0.47754230350255966\n","140 0.47894294932484627\n","141 0.48660991340875626\n","142 0.488581083714962\n","143 0.4928463287651539\n","144 0.49150148034095764\n","145 0.48822983354330063\n","146 0.48834366351366043\n","147 0.4873621389269829\n","148 0.48648626729846\n","149 0.4848336800932884\n","150 0.47983430698513985\n","151 0.47664599120616913\n","152 0.4726555161178112\n","153 0.46909722313284874\n","154 0.46698831766843796\n","155 0.46528226137161255\n","156 0.46321433037519455\n","157 0.4619538336992264\n","158 0.460846871137619\n","159 0.4607841372489929\n","160 0.46045638620853424\n","161 0.4623177796602249\n","162 0.4631737396121025\n","163 0.46429409831762314\n","164 0.4645248055458069\n","165 0.46506088972091675\n","166 0.4648688808083534\n","167 0.4643292911350727\n","168 0.4632193520665169\n","169 0.4649452976882458\n","170 0.4672025255858898\n","171 0.4696328565478325\n","172 0.47132231667637825\n","173 0.4669235721230507\n","174 0.4683797284960747\n","175 0.46838483959436417\n","176 0.46796393766999245\n","177 0.46924447268247604\n","178 0.46674537286162376\n","179 0.4669222943484783\n","180 0.4639505222439766\n","181 0.4610805884003639\n","182 0.459008876234293\n","183 0.4577641598880291\n","184 0.4550935700535774\n","185 0.45302216336131096\n","186 0.45178069174289703\n","187 0.45009759441018105\n","188 0.4502352513372898\n","189 0.4500095434486866\n","190 0.4517579898238182\n","191 0.4531557597219944\n","192 0.45653119683265686\n","193 0.45680511742830276\n","194 0.4554452858865261\n","195 0.45660101249814034\n","196 0.4545848071575165\n","197 0.45332229882478714\n","198 0.45054204016923904\n","199 0.45069609582424164\n","200 0.45417479425668716\n","201 0.4560972936451435\n","202 0.46113454550504684\n","203 0.4671938084065914\n","204 0.46593114361166954\n","205 0.4648674502968788\n","206 0.4599410332739353\n","207 0.4607387036085129\n","208 0.45643390715122223\n","209 0.4551147595047951\n","210 0.45117446407675743\n","211 0.4510244131088257\n","212 0.4481014981865883\n","213 0.4463866837322712\n","214 0.4450329579412937\n","215 0.4436389394104481\n","216 0.44264157488942146\n","217 0.44164587929844856\n","218 0.4414481818675995\n","219 0.44082290679216385\n","220 0.4408370181918144\n","221 0.4400246627628803\n","222 0.4409702271223068\n","223 0.44013088941574097\n","224 0.4402681589126587\n","225 0.4397291801869869\n","226 0.43937917426228523\n","227 0.43849748745560646\n","228 0.4383861608803272\n","229 0.4379512742161751\n","230 0.4376642182469368\n","231 0.4374704621732235\n","232 0.4372292160987854\n","233 0.4372032471001148\n","234 0.4368665888905525\n","235 0.43719665706157684\n","236 0.4378713220357895\n","237 0.4380662813782692\n","238 0.4393823854625225\n","239 0.4386582151055336\n","240 0.43683623149991035\n","241 0.43539759516716003\n","242 0.43456798419356346\n","243 0.43458758667111397\n","244 0.43656206130981445\n","245 0.4382004626095295\n","246 0.4395841434597969\n","247 0.44016387686133385\n","248 0.43803368881344795\n","249 0.43880782648921013\n","250 0.43704843521118164\n","251 0.4377232789993286\n","252 0.43762321397662163\n","253 0.44069208949804306\n","254 0.4420826993882656\n","255 0.4420138783752918\n","256 0.4425858072936535\n","257 0.44075848162174225\n","258 0.4389880783855915\n","259 0.4358150251209736\n","260 0.43335215374827385\n","261 0.43150878697633743\n","262 0.4302751198410988\n","263 0.429021917283535\n","264 0.4285675026476383\n","265 0.42833616212010384\n","266 0.4284440241754055\n","267 0.4284718744456768\n","268 0.4286320433020592\n","269 0.43060440197587013\n","270 0.43122461065649986\n","271 0.4333554804325104\n","272 0.4353680722415447\n","273 0.43543563038110733\n","274 0.43695708736777306\n","275 0.43434225395321846\n","276 0.43310143053531647\n","277 0.43164971098303795\n","278 0.43146035075187683\n","279 0.4291822612285614\n","280 0.42769455537199974\n","281 0.4270443916320801\n","282 0.42579008266329765\n","283 0.4258081689476967\n","284 0.4257339648902416\n","285 0.426319882273674\n","286 0.4273907020688057\n","287 0.42820995301008224\n","288 0.42717161774635315\n","289 0.4260890819132328\n","290 0.4251817651093006\n","291 0.425027746707201\n","292 0.4240530729293823\n","293 0.42443983629345894\n","294 0.42529164254665375\n","295 0.42586033046245575\n","296 0.42630043625831604\n","297 0.4275287315249443\n","298 0.4286496490240097\n","299 0.4283354878425598\n","300 0.4279864653944969\n","301 0.42703912407159805\n","302 0.42793064936995506\n","303 0.4249766580760479\n","304 0.4250430800020695\n","305 0.4244369603693485\n","306 0.4267968088388443\n","307 0.42526403814554214\n","308 0.4271388426423073\n","309 0.4269079193472862\n","310 0.43015122413635254\n","311 0.4309134781360626\n","312 0.43015408143401146\n","313 0.43319428712129593\n","314 0.44405926391482353\n","315 0.4429367929697037\n","316 0.4391747862100601\n","317 0.4364817924797535\n","318 0.43508657813072205\n","319 0.4360046796500683\n","320 0.4339475743472576\n","321 0.43401995673775673\n","322 0.4324232488870621\n","323 0.43377142772078514\n","324 0.43199703842401505\n","325 0.42989199608564377\n","326 0.43045248463749886\n","327 0.4274356737732887\n","328 0.4244317561388016\n","329 0.4223911836743355\n","330 0.4217278026044369\n","331 0.4198664650321007\n","332 0.4207259751856327\n","333 0.4200903847813606\n","334 0.42287691682577133\n","335 0.42229487001895905\n","336 0.42575211450457573\n","337 0.424946416169405\n","338 0.42480822652578354\n","339 0.4226297065615654\n","340 0.42129435017704964\n","341 0.4186205491423607\n","342 0.41738738864660263\n","343 0.41578835621476173\n","344 0.41435976698994637\n","345 0.4140266440808773\n","346 0.4129576124250889\n","347 0.4131712168455124\n","348 0.41283541172742844\n","349 0.4129161164164543\n","350 0.4128586873412132\n","351 0.4123946614563465\n","352 0.4125928319990635\n","353 0.41227009147405624\n","354 0.414302222430706\n","355 0.41448045521974564\n","356 0.4166187159717083\n","357 0.4190891571342945\n","358 0.41977883502840996\n","359 0.4218818470835686\n","360 0.42032844200730324\n","361 0.4194421321153641\n","362 0.4173659346997738\n","363 0.41579151153564453\n","364 0.41479360312223434\n","365 0.4143461547791958\n","366 0.41427429020404816\n","367 0.4166170172393322\n","368 0.41627923771739006\n","369 0.4188999645411968\n","370 0.41725748404860497\n","371 0.41891997307538986\n","372 0.41662755981087685\n","373 0.4156131260097027\n","374 0.4133325628936291\n","375 0.411981999874115\n","376 0.4112291522324085\n","377 0.41035690531134605\n","378 0.40938377007842064\n","379 0.409019835293293\n","380 0.4093896858394146\n","381 0.40858352929353714\n","382 0.40938112884759903\n","383 0.40923357382416725\n","384 0.4094117283821106\n","385 0.4102143980562687\n","386 0.4102623462677002\n","387 0.41226181015372276\n","388 0.41194578632712364\n","389 0.41253675520420074\n","390 0.4145956337451935\n","391 0.4148075394332409\n","392 0.4157717637717724\n","393 0.4144822768867016\n","394 0.4154495969414711\n","395 0.4158385284245014\n","396 0.41459011286497116\n","397 0.41348687186837196\n","398 0.4148482233285904\n","399 0.4164511673152447\n","400 0.4182530641555786\n","401 0.4219199828803539\n","402 0.42168863862752914\n","403 0.42367875576019287\n","404 0.42394766584038734\n","405 0.4290425069630146\n","406 0.4328371249139309\n","407 0.4326527751982212\n","408 0.4288877919316292\n","409 0.42931151390075684\n","410 0.4321352280676365\n","411 0.4309867136180401\n","412 0.4313671737909317\n","413 0.42855771258473396\n","414 0.42969683930277824\n","415 0.42431897297501564\n","416 0.4234563298523426\n","417 0.41907913982868195\n","418 0.4180123396217823\n","419 0.4152776598930359\n","420 0.4124971702694893\n","421 0.41073063760995865\n","422 0.40915706008672714\n","423 0.407828688621521\n","424 0.40780113637447357\n","425 0.4074202738702297\n","426 0.40601351484656334\n","427 0.40535855293273926\n","428 0.40530579164624214\n","429 0.4039350859820843\n","430 0.40467850118875504\n","431 0.4063411355018616\n","432 0.4063880182802677\n","433 0.40491772815585136\n","434 0.4041356332600117\n","435 0.4041177071630955\n","436 0.40408310294151306\n","437 0.40470412001013756\n","438 0.4060540944337845\n","439 0.4061196371912956\n","440 0.40723640099167824\n","441 0.4064907617866993\n","442 0.40658921003341675\n","443 0.4062656983733177\n","444 0.4071653448045254\n","445 0.4071928821504116\n","446 0.40719498693943024\n","447 0.40602609515190125\n","448 0.405904907733202\n","449 0.40572164952754974\n","450 0.4057711511850357\n","451 0.40637196972966194\n","452 0.40627939626574516\n","453 0.4055941812694073\n","454 0.4064725749194622\n","455 0.40465694293379784\n","456 0.40417130663990974\n","457 0.4037909507751465\n","458 0.4026303179562092\n","459 0.40249308198690414\n","460 0.40146613493561745\n","461 0.4015405476093292\n","462 0.4008459262549877\n","463 0.40053491294384\n","464 0.4004107229411602\n","465 0.39967745169997215\n","466 0.39978019520640373\n","467 0.4007844924926758\n","468 0.401397492736578\n","469 0.4018849618732929\n","470 0.40412623807787895\n","471 0.40618013963103294\n","472 0.4079430438578129\n","473 0.4097275920212269\n","474 0.40908772498369217\n","475 0.40917160362005234\n","476 0.40824100002646446\n","477 0.40645653009414673\n","478 0.40445438772439957\n","479 0.402345634996891\n","480 0.4014141708612442\n","481 0.4002336859703064\n","482 0.40020889416337013\n","483 0.39980823919177055\n","484 0.40030305460095406\n","485 0.4001402035355568\n","486 0.4003427177667618\n","487 0.4005126617848873\n","488 0.4019128158688545\n","489 0.4019903875887394\n","490 0.40283327922225\n","491 0.4023319408297539\n","492 0.4032321088016033\n","493 0.401656161993742\n","494 0.39999352395534515\n","495 0.39864446222782135\n","496 0.3975879028439522\n","497 0.3963479418307543\n","498 0.3959080260246992\n","499 0.3960217796266079\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'./gdrive/My Drive/homework3/model/model_lstm.ckpt'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"LWGSi3xFdT65","colab_type":"text"},"source":["Following is the validation step, where the trained model is used to calculate for the validation files and the SNR is calculated to observe the performance."]},{"cell_type":"code","metadata":{"id":"3x_wqczHNq35","colab_type":"code","outputId":"057a26c6-f332-4c6a-a958-78412a8218a9","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["val_tot = len(mval_X)\n","snr= [] \n","for i in range(0, val_tot, batch_size):\n","  end = i + batch_size if val_tot>i + batch_size else val_tot\n","  batch_x = mval_X[i:end]\n","\n","  m_pred = sess.run([dense], feed_dict={X: batch_x,keep_prob:1})\n","  #print(m_pred[0].shape)\n","  for j in range(i, end):\n","    x = val_X[j]\n","    s_val = mval_S[j][:val_X[j].shape[0], :]\n","    m = m_pred[0][j - i][:val_X[j].shape[0], :]\n","    m = 1*(m>0.5)\n","    cleaned = x * m\n","    snr.append(getSNR(snvv[j],librosa.istft(cleaned.T, hop_length=512)))\n","        \n","print(np.mean(snr))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["11.078615647458843\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DE-JH2aLdknA","colab_type":"text"},"source":["Finally, the test set of files are passed to the trained network, to get the desired IBM output, which is used to get the correct signal back."]},{"cell_type":"code","metadata":{"id":"GQnAC549VLT7","colab_type":"code","colab":{}},"source":["mtest_X = get_abs(test_X)\n","test_tot = len(test_X)\n","\n","for i in range(0, test_tot, batch_size):\n","  end = i + batch_size if test_tot>i + batch_size else test_tot\n","  \n","  batch_x = mtest_X[i:end]\n","   \n","  m_pred = sess.run([dense], feed_dict={X: batch_x,keep_prob:1})\n","  M_hat = m_pred[0]\n","  for j in range(i, end):\n","    x = test_X[j]\n","    s = mtest_X[j][:test_X[j].shape[0], :]\n","    m = M_hat[j - i][:test_X[j].shape[0], :]\n","    m = 1*(m>0.5)\n","    cleaned = x * m\n","    if not os.path.exists(path_result):\n","      os.makedirs(path_result)\n","    file = path_result + 'cleaned_test' + numtostring(j) + '.wav'\n","    s_hat = librosa.istft(cleaned.T, hop_length=512)\n","    librosa.output.write_wav(file, s_hat, sr=16000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBSMpiOZQZM3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}